{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/susumu.ota/synth-persona\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/susumu.ota/synth-persona/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-10 02:19:25,620\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%cd ~/synth-persona\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from abc import ABC, abstractmethod\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from litellm import batch_completion\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger.setLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(ABC):\n",
    "    def __init__(self, model: str, max_tokens=512, seed: int | None = None, temperature=1.0, top_p=0.95) -> None:\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.seed = seed\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        logger.debug(\n",
    "            f\"model: {model}, max_tokens: {max_tokens}, seed: {seed}, temperature: {temperature}, top_p: {top_p}\"\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, messages_batch: list[list[dict[str, str]]]) -> list[str]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class LiteLLMModel(LanguageModel):\n",
    "    def __init__(self, model: str, max_tokens=512, seed: int | None = None, temperature=1.0, top_p=0.95) -> None:\n",
    "        super().__init__(model, max_tokens, seed, temperature, top_p)\n",
    "\n",
    "    def __call__(self, messages_batch: list[list[dict[str, str]]]) -> list[str]:\n",
    "        contents = [\n",
    "            response.choices[0].message.content or \"\"\n",
    "            for response in batch_completion(\n",
    "                model=self.model,\n",
    "                messages=messages_batch,\n",
    "                max_tokens=self.max_tokens,\n",
    "                seed=self.seed,\n",
    "                temperature=self.temperature,\n",
    "                top_p=self.top_p,\n",
    "            )\n",
    "        ]\n",
    "        assert len(contents) == len(messages_batch)\n",
    "        return contents\n",
    "\n",
    "\n",
    "class VLLMModel(LanguageModel):\n",
    "    def __init__(self, model: str, max_tokens=512, seed: int | None = None, temperature=1.0, top_p=0.95) -> None:\n",
    "        super().__init__(model, max_tokens, seed, temperature, top_p)\n",
    "        self.vllm = LLM(model, seed=seed, gpu_memory_utilization=1.0, max_model_len=32 * 1024)  # TODO: parameterize\n",
    "        self.tokenizer = self.vllm.get_tokenizer()\n",
    "\n",
    "    def __call__(self, messages_batch: list[list[dict[str, str]]]) -> list[str]:\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=self.max_tokens, seed=self.seed, temperature=self.temperature, top_p=self.top_p\n",
    "        )\n",
    "        prompts = [\n",
    "            self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            for messages in messages_batch\n",
    "        ]\n",
    "        outputs = self.vllm.generate(prompts, sampling_params=sampling_params, use_tqdm=False)\n",
    "        contents = [output.outputs[0].text for output in outputs]\n",
    "        assert len(contents) == len(messages_batch)\n",
    "        return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.'\n",
    "SYSTEM_PROMPT_RENAME = 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[B]]\" if assistant B is better, \"[[A]]\" if assistant A is better, and \"[[C]]\" for a tie.'\n",
    "\n",
    "PROMPT_TEMPLATE = \"[User Question]\\n{question}\\n\\n[The Start of Assistant A's Answer]\\n{answer_a}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{answer_b}\\n[The End of Assistant B's Answer]\"\n",
    "PROMPT_TEMPLATE_RENAME = \"[User Question]\\n{question}\\n\\n[The Start of Assistant B's Answer]\\n{answer_a}\\n[The End of Assistant B's Answer]\\n\\n[The Start of Assistant A's Answer]\\n{answer_b}\\n[The End of Assistant A's Answer]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers need to be a list of dictionaries with keys \"question\", \"answer_a\", and \"answer_b\"\n",
    "def generate_decision_texts(\n",
    "    llm: LanguageModel, answers: list[dict[str, str]], system_prompt: str, prompt_template: str\n",
    ") -> list[str]:\n",
    "    prompts = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(**answer)},\n",
    "        ]\n",
    "        for answer in answers\n",
    "    ]\n",
    "    return llm(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_TEMPLATE = \"\"\"Create a math problem related to the following persona:\n",
    "\n",
    "{persona}\n",
    "\n",
    "Note:\n",
    "\n",
    "1. The math problem should be short, easy and involve basic mathematical skills and knowledge. Any average grade school student can solve it correctly.\n",
    "2. You should make full use of the persona description to create the math problem to ensure that the math problem is unique and specific to the persona.\n",
    "3. Your response should not include a solution to the created math problem.\n",
    "4. 深い専門知識が必要な問題を避け、平均的な知識と常識の範囲内で解ける問題にしてください。\n",
    "5. 簡潔に日本語で回答してください。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_answers(dataset_a: Dataset, dataset_b: Dataset, question_template: str) -> list[dict[str, str]]:\n",
    "    return [\n",
    "        {\n",
    "            \"question\": question_template.format(persona=a[\"persona\"]),\n",
    "            \"answer_a\": a[\"problem_answer\"],\n",
    "            \"answer_b\": b[\"problem_answer\"],\n",
    "        }\n",
    "        for a, b in zip(dataset_a, dataset_b)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(text: str) -> dict[str, str]:\n",
    "    m = re.match(r\"^(<think>)?(.*)</think>(.*)$\", text, flags=re.DOTALL | re.MULTILINE)\n",
    "    return (\n",
    "        {\"think\": m.group(2), \"answer\": m.group(3), \"format_reward\": 1.0}\n",
    "        if m\n",
    "        else {\"think\": \"\", \"answer\": text, \"format_reward\": 0.0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_decision_text(decision_text: str) -> str:\n",
    "    is_a = is_b = is_c = False\n",
    "    if \"[[A]]\" in decision_text:\n",
    "        is_a = True\n",
    "    if \"[[B]]\" in decision_text:\n",
    "        is_b = True\n",
    "    if \"[[C]]\" in decision_text:\n",
    "        is_c = True\n",
    "    decision = \"C\"\n",
    "    if is_a and is_b:\n",
    "        # raise ValueError(f\"Both A and B are chosen: {decision_text}\")\n",
    "        logger.debug(f\"Both A and B are chosen: {decision_text}\")\n",
    "        decision = \"C\"\n",
    "    elif is_a:\n",
    "        decision = \"A\"\n",
    "    elif is_b:\n",
    "        decision = \"B\"\n",
    "    elif is_c:\n",
    "        decision = \"C\"\n",
    "    else:\n",
    "        # raise ValueError(f\"Unknown decision: {decision_text}\")\n",
    "        logger.debug(f\"Unknown decision: {decision_text}\")\n",
    "        decision = \"C\"\n",
    "    # logger.debug(f\"decision: {decision}\")\n",
    "    return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_answers(\n",
    "    llm: LanguageModel, answers: list[dict[str, str]], system_prompt: str, prompt_template: str\n",
    ") -> list[str]:\n",
    "    return [\n",
    "        parse_decision_text(parse_response(decision_text)[\"answer\"])\n",
    "        for decision_text in generate_decision_texts(llm, answers, system_prompt, prompt_template)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_decisions(decisions: list[str]) -> dict[str, int]:\n",
    "    return {decision: decisions.count(decision) for decision in [\"A\", \"B\", \"C\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_answers(answers: list[dict[str, str]]) -> list[dict[str, str]]:\n",
    "    return [{\"question\": a[\"question\"], \"answer_a\": a[\"answer_b\"], \"answer_b\": a[\"answer_a\"]} for a in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_decisions(decisions: list[str]) -> list[str]:\n",
    "    return [\"A\" if d == \"B\" else \"B\" if d == \"A\" else \"C\" for d in decisions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_decisions(\n",
    "    decisions: list[str], swapped_decisions: list[str], is_position_swapped: bool\n",
    ") -> dict[str, dict[str, int]]:\n",
    "    count = {\"A\": 0, \"B\": 0, \"C\": 0, \"bias\": 0}\n",
    "    bias = {\"first\": 0, \"second\": 0}\n",
    "    for d, sd in zip(decisions, swapped_decisions):\n",
    "        if d == sd:\n",
    "            count[d] += 1\n",
    "        else:\n",
    "            count[\"bias\"] += 1\n",
    "            if d == \"A\" and (sd == \"B\" or sd == \"C\"):\n",
    "                # if is_position_swapped is True,  sd == \"B\" means that LLM prefers the first answer which is denoted as \"first\"\n",
    "                # if is_position_swapped is False, sd == \"B\" means that LLM prefers the name \"B\" which is denoted as \"second\"\n",
    "                bias[\"first\" if is_position_swapped else \"second\"] += 1\n",
    "            elif d == \"B\" and (sd == \"A\" or sd == \"C\"):\n",
    "                bias[\"second\" if is_position_swapped else \"first\"] += 1\n",
    "            elif d == \"C\" and sd == \"A\":\n",
    "                bias[\"second\" if is_position_swapped else \"first\"] += 1\n",
    "            elif d == \"C\" and sd == \"B\":\n",
    "                bias[\"first\" if is_position_swapped else \"second\"] += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown decision: {d}, {sd}\")\n",
    "    return {\"count\": count, \"bias\": bias}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unbiased_decisions(\n",
    "    decisions: list[str], position_swapped_decisions: list[str], name_swapped_decisions: list[str]\n",
    ") -> dict[str, dict[str, int]]:\n",
    "    assert len(decisions) == len(position_swapped_decisions) == len(name_swapped_decisions)\n",
    "    return [\n",
    "        d if d == pd == nd else \"C\" for d, pd, nd in zip(decisions, position_swapped_decisions, name_swapped_decisions)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias(comparison):\n",
    "    total = sum(comparison[\"count\"].values())\n",
    "    bias = comparison[\"count\"][\"bias\"]\n",
    "    first = comparison[\"bias\"][\"first\"]\n",
    "    second = comparison[\"bias\"][\"second\"]\n",
    "    assert bias == first + second\n",
    "    return {\n",
    "        \"consistent\": (total - bias) / total,\n",
    "        \"first\": first / total,\n",
    "        \"second\": second / total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_rate(decisions: list[str]) -> dict[str, float]:\n",
    "    count = count_decisions(decisions)\n",
    "    total = sum(count.values())\n",
    "    decision_a = count.get(\"A\", 0)\n",
    "    decision_b = count.get(\"B\", 0)\n",
    "    decision_c = count.get(\"C\", 0)\n",
    "    assert total == decision_a + decision_b + decision_c\n",
    "    return {\n",
    "        \"A\": decision_a / (total - decision_c),\n",
    "        \"B\": decision_b / (total - decision_c),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_once(judge_llm: LanguageModel, answers: list[dict[str, str]]) -> dict[str, dict]:\n",
    "    logger.debug(\"judging baseline...\")\n",
    "    time_start = time()\n",
    "    decisions = judge_answers(judge_llm, answers, SYSTEM_PROMPT, PROMPT_TEMPLATE)\n",
    "    logger.debug(f\"judging baseline...done: {time() - time_start:.2f}s\")\n",
    "\n",
    "    logger.debug(\"judging position swapped...\")\n",
    "    time_start = time()\n",
    "    position_swapped_decisions = swap_decisions(\n",
    "        judge_answers(judge_llm, swap_answers(answers), SYSTEM_PROMPT, PROMPT_TEMPLATE)\n",
    "    )\n",
    "    logger.debug(f\"judging position swapped...done: {time() - time_start:.2f}s\")\n",
    "\n",
    "    logger.debug(\"judging name swapped...\")\n",
    "    time_start = time()\n",
    "    name_swapped_decisions = swap_decisions(\n",
    "        judge_answers(judge_llm, answers, SYSTEM_PROMPT_RENAME, PROMPT_TEMPLATE_RENAME)\n",
    "    )\n",
    "    logger.debug(f\"judging name swapped...done: {time() - time_start:.2f}s\")\n",
    "\n",
    "    unbiased_decisions = get_unbiased_decisions(decisions, position_swapped_decisions, name_swapped_decisions)\n",
    "\n",
    "    position_comparison = compare_decisions(decisions, position_swapped_decisions, True)\n",
    "    name_comparison = compare_decisions(decisions, name_swapped_decisions, False)\n",
    "\n",
    "    position_bias = get_bias(position_comparison)\n",
    "    name_bias = get_bias(name_comparison)\n",
    "\n",
    "    return {\n",
    "        \"decisions\": decisions,\n",
    "        \"position_swapped_decisions\": position_swapped_decisions,\n",
    "        \"name_swapped_decisions\": name_swapped_decisions,\n",
    "        \"unbiased_decisions\": unbiased_decisions,\n",
    "        \"decisions_count\": count_decisions(decisions),\n",
    "        \"position_swapped_decisions_count\": count_decisions(position_swapped_decisions),\n",
    "        \"name_swapped_decisions_count\": count_decisions(name_swapped_decisions),\n",
    "        \"unbiased_decisions_count\": count_decisions(unbiased_decisions),\n",
    "        \"decisions_win_rate\": get_win_rate(decisions),\n",
    "        \"position_swapped_decisions_win_rate\": get_win_rate(position_swapped_decisions),\n",
    "        \"name_swapped_decisions_win_rate\": get_win_rate(name_swapped_decisions),\n",
    "        \"unbiased_decisions_win_rate\": get_win_rate(unbiased_decisions),\n",
    "        \"position_comparison\": position_comparison,\n",
    "        \"name_comparison\": name_comparison,\n",
    "        \"position_bias\": position_bias,\n",
    "        \"name_bias\": name_bias,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(judge_model: str, input_1_jsonl: str, input_2_jsonl: str, n: int) -> dict[str, pd.DataFrame]:\n",
    "    judge_llm = LiteLLMModel(judge_model, max_tokens=4096, seed=1, temperature=0.6, top_p=0.95)\n",
    "\n",
    "    dataset_1 = load_dataset(\"json\", data_files=input_1_jsonl, split=\"train\", cache_dir=\".cache\")\n",
    "    dataset_2 = load_dataset(\"json\", data_files=input_2_jsonl, split=\"train\", cache_dir=\".cache\")\n",
    "    answers = convert_answers(dataset_1, dataset_2, QUESTION_TEMPLATE)\n",
    "\n",
    "    results = [run_experiment_once(judge_llm, answers) for _ in tqdm(range(n))]\n",
    "    return {key: pd.DataFrame([result[key] for result in results]) for key in results[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:04:49 - DEBUG - __main__ - model: deepseek/deepseek-reasoner, max_tokens: 4096, seed: 1, temperature: 0.6, top_p: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:04:50 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-09 11:07:05 - DEBUG - __main__ - judging baseline...done: 134.66s\n",
      "2025-03-09 11:07:05 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-09 11:09:24 - DEBUG - __main__ - judging position swapped...done: 139.25s\n",
      "2025-03-09 11:09:24 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-09 11:11:28 - DEBUG - __main__ - judging name swapped...done: 124.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [06:38<46:26, 398.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:11:28 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-09 11:13:19 - DEBUG - __main__ - judging baseline...done: 110.63s\n",
      "2025-03-09 11:13:19 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-09 11:14:56 - DEBUG - __main__ - judging position swapped...done: 97.17s\n",
      "2025-03-09 11:14:56 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-09 11:16:44 - DEBUG - __main__ - judging name swapped...done: 107.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [11:53<34:56, 349.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:16:44 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-09 11:19:27 - DEBUG - __main__ - judging baseline...done: 163.50s\n",
      "2025-03-09 11:19:27 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-09 11:21:35 - DEBUG - __main__ - judging position swapped...done: 128.06s\n",
      "2025-03-09 11:21:35 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-09 11:23:30 - DEBUG - __main__ - judging name swapped...done: 114.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [18:39<31:16, 375.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:23:30 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-09 11:25:15 - DEBUG - __main__ - judging baseline...done: 104.95s\n",
      "2025-03-09 11:25:15 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-09 11:27:18 - DEBUG - __main__ - judging position swapped...done: 122.85s\n",
      "2025-03-09 11:27:18 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-09 11:29:34 - DEBUG - __main__ - judging name swapped...done: 136.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [24:43<24:43, 370.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:29:34 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-09 11:31:29 - DEBUG - __main__ - judging baseline...done: 115.24s\n",
      "2025-03-09 11:31:29 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-09 11:33:28 - DEBUG - __main__ - judging position swapped...done: 119.34s\n",
      "2025-03-09 11:33:28 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-09 11:35:29 - DEBUG - __main__ - judging name swapped...done: 120.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [30:38<18:15, 365.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:35:29 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-09 11:37:37 - DEBUG - __main__ - judging baseline...done: 128.42s\n",
      "2025-03-09 11:37:37 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-09 11:39:18 - DEBUG - __main__ - judging position swapped...done: 100.85s\n",
      "2025-03-09 11:39:18 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-09 11:41:30 - DEBUG - __main__ - judging name swapped...done: 132.31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [36:40<12:07, 363.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:41:30 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-09 11:43:44 - DEBUG - __main__ - judging baseline...done: 133.89s\n",
      "2025-03-09 11:43:44 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-09 11:46:00 - DEBUG - __main__ - judging position swapped...done: 135.52s\n",
      "2025-03-09 11:46:00 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-09 11:48:18 - DEBUG - __main__ - judging name swapped...done: 138.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [43:27<06:18, 378.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 11:48:18 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-09 11:50:20 - DEBUG - __main__ - judging baseline...done: 121.43s\n",
      "2025-03-09 11:50:20 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-09 11:52:01 - DEBUG - __main__ - judging position swapped...done: 101.64s\n",
      "2025-03-09 11:52:01 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-09 11:53:55 - DEBUG - __main__ - judging name swapped...done: 113.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [49:04<00:00, 368.06s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decisions':   0  1  2  3  4  5  6  7  8  9   ... 90 91 92 93 94 95 96 97 98 99\n",
       " 0  A  A  B  B  B  A  B  A  A  B  ...  A  A  A  A  B  A  B  A  B  B\n",
       " 1  A  A  B  B  B  B  B  B  A  A  ...  A  A  A  A  B  A  B  A  A  B\n",
       " 2  A  B  B  B  B  A  C  A  A  B  ...  A  A  A  B  A  A  B  A  A  B\n",
       " 3  A  B  B  B  A  A  B  A  A  A  ...  A  A  A  A  B  A  B  A  B  B\n",
       " 4  A  A  A  B  A  A  C  B  A  A  ...  A  A  A  B  B  A  B  A  B  B\n",
       " 5  A  A  B  B  A  A  B  A  A  A  ...  A  A  A  A  A  A  B  A  A  B\n",
       " 6  A  A  A  B  A  A  B  B  A  C  ...  A  A  A  B  B  B  B  A  A  B\n",
       " 7  A  A  A  B  A  A  A  B  A  B  ...  A  A  A  A  B  A  B  A  B  B\n",
       " \n",
       " [8 rows x 100 columns],\n",
       " 'position_swapped_decisions':   0  1  2  3  4  5  6  7  8  9   ... 90 91 92 93 94 95 96 97 98 99\n",
       " 0  B  B  B  B  B  B  B  B  A  B  ...  B  A  A  B  B  A  B  B  B  B\n",
       " 1  A  B  B  B  B  B  B  B  A  B  ...  A  A  A  A  B  B  B  B  B  B\n",
       " 2  B  B  B  B  B  B  C  B  A  B  ...  B  A  A  B  B  B  B  B  B  B\n",
       " 3  B  B  B  B  B  B  B  B  A  B  ...  B  A  C  B  B  B  B  B  B  B\n",
       " 4  A  B  A  B  B  B  B  B  A  B  ...  B  A  A  B  B  B  B  B  B  B\n",
       " 5  B  B  A  B  B  B  B  B  B  B  ...  B  A  A  B  B  B  B  A  B  B\n",
       " 6  B  B  B  B  B  B  B  B  A  B  ...  A  A  A  B  B  A  B  A  B  B\n",
       " 7  B  B  A  B  B  B  B  B  A  B  ...  B  A  A  B  B  B  B  B  B  B\n",
       " \n",
       " [8 rows x 100 columns],\n",
       " 'name_swapped_decisions':   0  1  2  3  4  5  6  7  8  9   ... 90 91 92 93 94 95 96 97 98 99\n",
       " 0  A  A  A  B  A  A  A  A  A  B  ...  A  A  A  A  A  A  B  A  A  B\n",
       " 1  A  A  A  B  A  A  A  A  A  A  ...  A  A  A  A  A  A  B  A  A  B\n",
       " 2  A  A  A  B  A  A  A  A  A  B  ...  A  A  A  A  A  A  B  A  A  B\n",
       " 3  A  A  A  B  A  A  A  A  A  B  ...  A  A  A  A  A  A  B  A  A  B\n",
       " 4  A  A  A  B  A  A  C  A  A  B  ...  A  A  A  A  B  A  B  A  A  B\n",
       " 5  A  A  A  B  A  A  A  B  A  A  ...  A  A  A  A  A  A  B  A  A  B\n",
       " 6  A  A  A  B  A  A  B  B  A  B  ...  A  A  A  A  A  B  B  A  A  B\n",
       " 7  A  A  A  B  A  A  A  A  A  A  ...  A  A  A  A  A  A  C  A  A  B\n",
       " \n",
       " [8 rows x 100 columns],\n",
       " 'unbiased_decisions':   0  1  2  3  4  5  6  7  8  9   ... 90 91 92 93 94 95 96 97 98 99\n",
       " 0  C  C  C  B  C  C  C  C  A  B  ...  C  A  A  C  C  A  B  C  C  B\n",
       " 1  A  C  C  B  C  C  C  C  A  C  ...  A  A  A  A  C  C  B  C  C  B\n",
       " 2  C  C  C  B  C  C  C  C  A  B  ...  C  A  A  C  C  C  B  C  C  B\n",
       " 3  C  C  C  B  C  C  C  C  A  C  ...  C  A  C  C  C  C  B  C  C  B\n",
       " 4  A  C  A  B  C  C  C  C  A  C  ...  C  A  A  C  B  C  B  C  C  B\n",
       " 5  C  C  C  B  C  C  C  C  C  C  ...  C  A  A  C  C  C  B  A  C  B\n",
       " 6  C  C  C  B  C  C  B  B  A  C  ...  A  A  A  C  C  C  B  A  C  B\n",
       " 7  C  C  A  B  C  C  C  C  A  C  ...  C  A  A  C  C  C  C  C  C  B\n",
       " \n",
       " [8 rows x 100 columns],\n",
       " 'decisions_count':     A   B  C\n",
       " 0  66  31  3\n",
       " 1  70  26  4\n",
       " 2  66  26  8\n",
       " 3  72  24  4\n",
       " 4  71  24  5\n",
       " 5  77  20  3\n",
       " 6  77  20  3\n",
       " 7  74  23  3,\n",
       " 'position_swapped_decisions_count':     A   B  C\n",
       " 0  34  62  4\n",
       " 1  39  57  4\n",
       " 2  40  54  6\n",
       " 3  40  56  4\n",
       " 4  39  57  4\n",
       " 5  39  58  3\n",
       " 6  46  50  4\n",
       " 7  37  59  4,\n",
       " 'name_swapped_decisions_count':     A   B    C\n",
       " 0  88  12  NaN\n",
       " 1  89  11  NaN\n",
       " 2  87  12  1.0\n",
       " 3  87  12  1.0\n",
       " 4  82  16  2.0\n",
       " 5  88  12  NaN\n",
       " 6  82  15  3.0\n",
       " 7  87  11  2.0,\n",
       " 'unbiased_decisions_count':     A   B   C\n",
       " 0  31  11  58\n",
       " 1  37   8  55\n",
       " 2  35  10  55\n",
       " 3  35  10  55\n",
       " 4  38  14  48\n",
       " 5  37  10  53\n",
       " 6  45  11  44\n",
       " 7  36   9  55,\n",
       " 'decisions_win_rate':           A         B\n",
       " 0  0.680412  0.319588\n",
       " 1  0.729167  0.270833\n",
       " 2  0.717391  0.282609\n",
       " 3  0.750000  0.250000\n",
       " 4  0.747368  0.252632\n",
       " 5  0.793814  0.206186\n",
       " 6  0.793814  0.206186\n",
       " 7  0.762887  0.237113,\n",
       " 'position_swapped_decisions_win_rate':           A         B\n",
       " 0  0.354167  0.645833\n",
       " 1  0.406250  0.593750\n",
       " 2  0.425532  0.574468\n",
       " 3  0.416667  0.583333\n",
       " 4  0.406250  0.593750\n",
       " 5  0.402062  0.597938\n",
       " 6  0.479167  0.520833\n",
       " 7  0.385417  0.614583,\n",
       " 'name_swapped_decisions_win_rate':           A         B\n",
       " 0  0.880000  0.120000\n",
       " 1  0.890000  0.110000\n",
       " 2  0.878788  0.121212\n",
       " 3  0.878788  0.121212\n",
       " 4  0.836735  0.163265\n",
       " 5  0.880000  0.120000\n",
       " 6  0.845361  0.154639\n",
       " 7  0.887755  0.112245,\n",
       " 'unbiased_decisions_win_rate':           A         B\n",
       " 0  0.738095  0.261905\n",
       " 1  0.822222  0.177778\n",
       " 2  0.777778  0.222222\n",
       " 3  0.777778  0.222222\n",
       " 4  0.730769  0.269231\n",
       " 5  0.787234  0.212766\n",
       " 6  0.803571  0.196429\n",
       " 7  0.800000  0.200000,\n",
       " 'position_comparison':                                     count                        bias\n",
       " 0  {'A': 31, 'B': 28, 'C': 1, 'bias': 40}  {'first': 37, 'second': 3}\n",
       " 1  {'A': 37, 'B': 25, 'C': 1, 'bias': 37}  {'first': 35, 'second': 2}\n",
       " 2  {'A': 36, 'B': 23, 'C': 4, 'bias': 37}  {'first': 33, 'second': 4}\n",
       " 3  {'A': 35, 'B': 21, 'C': 2, 'bias': 42}  {'first': 37, 'second': 5}\n",
       " 4  {'A': 38, 'B': 22, 'C': 3, 'bias': 37}  {'first': 35, 'second': 2}\n",
       " 5  {'A': 37, 'B': 18, 'C': 1, 'bias': 44}  {'first': 41, 'second': 3}\n",
       " 6  {'A': 45, 'B': 19, 'C': 2, 'bias': 34}  {'first': 33, 'second': 1}\n",
       " 7  {'A': 36, 'B': 23, 'C': 0, 'bias': 41}  {'first': 40, 'second': 1},\n",
       " 'name_comparison':                                     count                        bias\n",
       " 0  {'A': 65, 'B': 11, 'C': 0, 'bias': 24}  {'first': 23, 'second': 1}\n",
       " 1   {'A': 67, 'B': 8, 'C': 0, 'bias': 25}  {'first': 22, 'second': 3}\n",
       " 2  {'A': 65, 'B': 11, 'C': 1, 'bias': 23}  {'first': 22, 'second': 1}\n",
       " 3  {'A': 70, 'B': 10, 'C': 0, 'bias': 20}  {'first': 18, 'second': 2}\n",
       " 4  {'A': 69, 'B': 14, 'C': 1, 'bias': 16}  {'first': 14, 'second': 2}\n",
       " 5  {'A': 75, 'B': 10, 'C': 0, 'bias': 15}  {'first': 13, 'second': 2}\n",
       " 6  {'A': 75, 'B': 12, 'C': 1, 'bias': 12}   {'first': 9, 'second': 3}\n",
       " 7   {'A': 72, 'B': 9, 'C': 1, 'bias': 18}  {'first': 16, 'second': 2},\n",
       " 'position_bias':    consistent  first  second\n",
       " 0        0.60   0.37    0.03\n",
       " 1        0.63   0.35    0.02\n",
       " 2        0.63   0.33    0.04\n",
       " 3        0.58   0.37    0.05\n",
       " 4        0.63   0.35    0.02\n",
       " 5        0.56   0.41    0.03\n",
       " 6        0.66   0.33    0.01\n",
       " 7        0.59   0.40    0.01,\n",
       " 'name_bias':    consistent  first  second\n",
       " 0        0.76   0.23    0.01\n",
       " 1        0.75   0.22    0.03\n",
       " 2        0.77   0.22    0.01\n",
       " 3        0.80   0.18    0.02\n",
       " 4        0.84   0.14    0.02\n",
       " 5        0.85   0.13    0.02\n",
       " 6        0.88   0.09    0.03\n",
       " 7        0.82   0.16    0.02}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"DEEPSEEK_API_BASE\"] = \"http://192.168.11.25:39877/v1\"\n",
    "judge_model = \"deepseek/deepseek-reasoner\"\n",
    "input_1_jsonl = \"misc/data/output-r1-46023.jsonl\"\n",
    "input_2_jsonl = \"misc/data/output-gpt-4o-mini-46023.jsonl\"\n",
    "n = 8\n",
    "\n",
    "dfs = run_experiment(judge_model, input_1_jsonl, input_2_jsonl, n)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['decisions', 'position_swapped_decisions', 'name_swapped_decisions', 'unbiased_decisions', 'decisions_count', 'position_swapped_decisions_count', 'name_swapped_decisions_count', 'unbiased_decisions_count', 'decisions_win_rate', 'position_swapped_decisions_win_rate', 'name_swapped_decisions_win_rate', 'unbiased_decisions_win_rate', 'position_comparison', 'name_comparison', 'position_bias', 'name_bias'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"misc/data\"\n",
    "prefix = \"results-r1-46023-vs-gpt-4o-mini-46023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for key, df in dfs.items():\n",
    "    df.to_csv(os.path.join(output_dir, f\"{prefix}-{key}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the saved results\n",
    "dfs = {}\n",
    "prefix = \"results-r1-46023-vs-gpt-4o-mini-46023\"\n",
    "for key in [\n",
    "    \"decisions\",\n",
    "    \"position_swapped_decisions\",\n",
    "    \"name_swapped_decisions\",\n",
    "    \"unbiased_decisions\",\n",
    "    \"decisions_count\",\n",
    "    \"position_swapped_decisions_count\",\n",
    "    \"name_swapped_decisions_count\",\n",
    "    \"unbiased_decisions_count\",\n",
    "    \"decisions_win_rate\",\n",
    "    \"position_swapped_decisions_win_rate\",\n",
    "    \"name_swapped_decisions_win_rate\",\n",
    "    \"unbiased_decisions_win_rate\",\n",
    "    \"position_comparison\",\n",
    "    \"name_comparison\",\n",
    "    \"position_bias\",\n",
    "    \"name_bias\",\n",
    "]:\n",
    "    dfs[key] = pd.read_csv(os.path.join(output_dir, f\"{prefix}-{key}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.261905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.177778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.196429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B\n",
       "0  0.738095  0.261905\n",
       "1  0.822222  0.177778\n",
       "2  0.777778  0.222222\n",
       "3  0.777778  0.222222\n",
       "4  0.730769  0.269231\n",
       "5  0.787234  0.212766\n",
       "6  0.803571  0.196429\n",
       "7  0.800000  0.200000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"unbiased_decisions_win_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(A    0.779681\n",
       " B    0.220319\n",
       " dtype: float64,\n",
       " A    0.031576\n",
       " B    0.031576\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"unbiased_decisions_win_rate\"].mean(), dfs[\"unbiased_decisions_win_rate\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(consistent    0.61000\n",
       " first         0.36375\n",
       " second        0.02625\n",
       " dtype: float64,\n",
       " consistent    0.032950\n",
       " first         0.029731\n",
       " second        0.014079\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"position_bias\"].mean(), dfs[\"position_bias\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(consistent    0.80875\n",
       " first         0.17125\n",
       " second        0.02000\n",
       " dtype: float64,\n",
       " consistent    0.046733\n",
       " first         0.050267\n",
       " second        0.007559\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"name_bias\"].mean(), dfs[\"name_bias\"].std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
