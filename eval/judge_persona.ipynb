{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/susumu.ota/synth-persona\n"
     ]
    }
   ],
   "source": [
    "%cd ~/synth-persona\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from abc import ABC, abstractmethod\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from litellm import batch_completion\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger.setLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(ABC):\n",
    "    def __init__(self, model: str, max_tokens=512, seed: int | None = None, temperature=1.0, top_p=0.95) -> None:\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        self.seed = seed\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        logger.debug(\n",
    "            f\"model: {model}, max_tokens: {max_tokens}, seed: {seed}, temperature: {temperature}, top_p: {top_p}\"\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, messages_batch: list[list[dict[str, str]]]) -> list[str]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class LiteLLMModel(LanguageModel):\n",
    "    def __init__(self, model: str, max_tokens=512, seed: int | None = None, temperature=1.0, top_p=0.95) -> None:\n",
    "        super().__init__(model, max_tokens, seed, temperature, top_p)\n",
    "\n",
    "    def __call__(self, messages_batch: list[list[dict[str, str]]]) -> list[str]:\n",
    "        contents = [\n",
    "            response.choices[0].message.content or \"\"\n",
    "            for response in batch_completion(\n",
    "                model=self.model,\n",
    "                messages=messages_batch,\n",
    "                max_tokens=self.max_tokens,\n",
    "                seed=self.seed,\n",
    "                temperature=self.temperature,\n",
    "                top_p=self.top_p,\n",
    "            )\n",
    "        ]\n",
    "        assert len(contents) == len(messages_batch)\n",
    "        return contents\n",
    "\n",
    "\n",
    "class VLLMModel(LanguageModel):\n",
    "    def __init__(self, model: str, max_tokens=512, seed: int | None = None, temperature=1.0, top_p=0.95) -> None:\n",
    "        super().__init__(model, max_tokens, seed, temperature, top_p)\n",
    "        self.vllm = LLM(model, seed=seed, gpu_memory_utilization=1.0, max_model_len=32 * 1024)  # TODO: parameterize\n",
    "        self.tokenizer = self.vllm.get_tokenizer()\n",
    "\n",
    "    def __call__(self, messages_batch: list[list[dict[str, str]]]) -> list[str]:\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=self.max_tokens, seed=self.seed, temperature=self.temperature, top_p=self.top_p\n",
    "        )\n",
    "        prompts = [\n",
    "            self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            for messages in messages_batch\n",
    "        ]\n",
    "        outputs = self.vllm.generate(prompts, sampling_params=sampling_params, use_tqdm=False)\n",
    "        contents = [output.outputs[0].text for output in outputs]\n",
    "        assert len(contents) == len(messages_batch)\n",
    "        return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.'\n",
    "SYSTEM_PROMPT_RENAME = 'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[B]]\" if assistant B is better, \"[[A]]\" if assistant A is better, and \"[[C]]\" for a tie.'\n",
    "\n",
    "PROMPT_TEMPLATE = \"[User Question]\\n{question}\\n\\n[The Start of Assistant A's Answer]\\n{answer_a}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{answer_b}\\n[The End of Assistant B's Answer]\"\n",
    "PROMPT_TEMPLATE_RENAME = \"[User Question]\\n{question}\\n\\n[The Start of Assistant B's Answer]\\n{answer_a}\\n[The End of Assistant B's Answer]\\n\\n[The Start of Assistant A's Answer]\\n{answer_b}\\n[The End of Assistant A's Answer]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers need to be a list of dictionaries with keys \"question\", \"answer_a\", and \"answer_b\"\n",
    "def generate_decision_texts(\n",
    "    llm: LanguageModel, answers: list[dict[str, str]], system_prompt: str, prompt_template: str\n",
    ") -> list[str]:\n",
    "    prompts = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(**answer)},\n",
    "        ]\n",
    "        for answer in answers\n",
    "    ]\n",
    "    return llm(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_answers(dataset_a: Dataset, dataset_b: Dataset, question_template: str) -> list[dict[str, str]]:\n",
    "#     return [\n",
    "#         {\n",
    "#             \"question\": question_template.format(persona=a[\"persona\"]),\n",
    "#             \"answer_a\": a[\"problem_answer\"],\n",
    "#             \"answer_b\": b[\"problem_answer\"],\n",
    "#         }\n",
    "#         for a, b in zip(dataset_a, dataset_b)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(text: str) -> dict[str, str]:\n",
    "    m = re.match(r\"^(<think>)?(.*)</think>(.*)$\", text, flags=re.DOTALL | re.MULTILINE)\n",
    "    return (\n",
    "        {\"think\": m.group(2), \"answer\": m.group(3), \"format_reward\": 1.0}\n",
    "        if m\n",
    "        else {\"think\": \"\", \"answer\": text, \"format_reward\": 0.0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_decision_text(decision_text: str) -> str:\n",
    "    is_a = is_b = is_c = False\n",
    "    if \"[[A]]\" in decision_text:\n",
    "        is_a = True\n",
    "    if \"[[B]]\" in decision_text:\n",
    "        is_b = True\n",
    "    if \"[[C]]\" in decision_text:\n",
    "        is_c = True\n",
    "    decision = \"C\"\n",
    "    if is_a and is_b:\n",
    "        # raise ValueError(f\"Both A and B are chosen: {decision_text}\")\n",
    "        logger.debug(f\"Both A and B are chosen: {decision_text}\")\n",
    "        decision = \"C\"\n",
    "    elif is_a:\n",
    "        decision = \"A\"\n",
    "    elif is_b:\n",
    "        decision = \"B\"\n",
    "    elif is_c:\n",
    "        decision = \"C\"\n",
    "    else:\n",
    "        # raise ValueError(f\"Unknown decision: {decision_text}\")\n",
    "        logger.debug(f\"Unknown decision: {decision_text}\")\n",
    "        decision = \"C\"\n",
    "    # logger.debug(f\"decision: {decision}\")\n",
    "    return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_answers(\n",
    "    llm: LanguageModel, answers: list[dict[str, str]], system_prompt: str, prompt_template: str\n",
    ") -> list[str]:\n",
    "    return [\n",
    "        parse_decision_text(parse_response(decision_text)[\"answer\"])\n",
    "        for decision_text in generate_decision_texts(llm, answers, system_prompt, prompt_template)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_decisions(decisions: list[str]) -> dict[str, int]:\n",
    "    return {decision: decisions.count(decision) for decision in [\"A\", \"B\", \"C\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_answers(answers: list[dict[str, str]]) -> list[dict[str, str]]:\n",
    "    return [{\"question\": a[\"question\"], \"answer_a\": a[\"answer_b\"], \"answer_b\": a[\"answer_a\"]} for a in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_decisions(decisions: list[str]) -> list[str]:\n",
    "    return [\"A\" if d == \"B\" else \"B\" if d == \"A\" else \"C\" for d in decisions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_decisions(\n",
    "    decisions: list[str], swapped_decisions: list[str], is_position_swapped: bool\n",
    ") -> dict[str, dict[str, int]]:\n",
    "    count = {\"A\": 0, \"B\": 0, \"C\": 0, \"bias\": 0}\n",
    "    bias = {\"first\": 0, \"second\": 0}\n",
    "    for d, sd in zip(decisions, swapped_decisions):\n",
    "        if d == sd:\n",
    "            count[d] += 1\n",
    "        else:\n",
    "            count[\"bias\"] += 1\n",
    "            if d == \"A\" and (sd == \"B\" or sd == \"C\"):\n",
    "                # if is_position_swapped is True,  sd == \"B\" means that LLM prefers the first answer which is denoted as \"first\"\n",
    "                # if is_position_swapped is False, sd == \"B\" means that LLM prefers the name \"B\" which is denoted as \"second\"\n",
    "                bias[\"first\" if is_position_swapped else \"second\"] += 1\n",
    "            elif d == \"B\" and (sd == \"A\" or sd == \"C\"):\n",
    "                bias[\"second\" if is_position_swapped else \"first\"] += 1\n",
    "            elif d == \"C\" and sd == \"A\":\n",
    "                bias[\"second\" if is_position_swapped else \"first\"] += 1\n",
    "            elif d == \"C\" and sd == \"B\":\n",
    "                bias[\"first\" if is_position_swapped else \"second\"] += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown decision: {d}, {sd}\")\n",
    "    return {\"count\": count, \"bias\": bias}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unbiased_decisions(\n",
    "    decisions: list[str], position_swapped_decisions: list[str], name_swapped_decisions: list[str]\n",
    ") -> dict[str, dict[str, int]]:\n",
    "    assert len(decisions) == len(position_swapped_decisions) == len(name_swapped_decisions)\n",
    "    return [\n",
    "        d if d == pd == nd else \"C\" for d, pd, nd in zip(decisions, position_swapped_decisions, name_swapped_decisions)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias(comparison):\n",
    "    total = sum(comparison[\"count\"].values())\n",
    "    bias = comparison[\"count\"][\"bias\"]\n",
    "    first = comparison[\"bias\"][\"first\"]\n",
    "    second = comparison[\"bias\"][\"second\"]\n",
    "    assert bias == first + second\n",
    "    return {\n",
    "        \"consistent\": (total - bias) / total,\n",
    "        \"first\": first / total,\n",
    "        \"second\": second / total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_rate(decisions: list[str]) -> dict[str, float]:\n",
    "    count = count_decisions(decisions)\n",
    "    total = sum(count.values())\n",
    "    decision_a = count.get(\"A\", 0)\n",
    "    decision_b = count.get(\"B\", 0)\n",
    "    decision_c = count.get(\"C\", 0)\n",
    "    assert total == decision_a + decision_b + decision_c\n",
    "    return {\n",
    "        \"A\": decision_a / (total - decision_c),\n",
    "        \"B\": decision_b / (total - decision_c),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_once(judge_llm: LanguageModel, answers: list[dict[str, str]]) -> dict[str, dict]:\n",
    "    logger.debug(\"judging baseline...\")\n",
    "    time_start = time()\n",
    "    decisions = judge_answers(judge_llm, answers, SYSTEM_PROMPT, PROMPT_TEMPLATE)\n",
    "    logger.debug(f\"judging baseline...done: {time() - time_start:.2f}s\")\n",
    "\n",
    "    logger.debug(\"judging position swapped...\")\n",
    "    time_start = time()\n",
    "    position_swapped_decisions = swap_decisions(\n",
    "        judge_answers(judge_llm, swap_answers(answers), SYSTEM_PROMPT, PROMPT_TEMPLATE)\n",
    "    )\n",
    "    logger.debug(f\"judging position swapped...done: {time() - time_start:.2f}s\")\n",
    "\n",
    "    logger.debug(\"judging name swapped...\")\n",
    "    time_start = time()\n",
    "    name_swapped_decisions = swap_decisions(\n",
    "        judge_answers(judge_llm, answers, SYSTEM_PROMPT_RENAME, PROMPT_TEMPLATE_RENAME)\n",
    "    )\n",
    "    logger.debug(f\"judging name swapped...done: {time() - time_start:.2f}s\")\n",
    "\n",
    "    unbiased_decisions = get_unbiased_decisions(decisions, position_swapped_decisions, name_swapped_decisions)\n",
    "\n",
    "    position_comparison = compare_decisions(decisions, position_swapped_decisions, True)\n",
    "    name_comparison = compare_decisions(decisions, name_swapped_decisions, False)\n",
    "\n",
    "    position_bias = get_bias(position_comparison)\n",
    "    name_bias = get_bias(name_comparison)\n",
    "\n",
    "    return {\n",
    "        \"decisions\": decisions,\n",
    "        \"position_swapped_decisions\": position_swapped_decisions,\n",
    "        \"name_swapped_decisions\": name_swapped_decisions,\n",
    "        \"unbiased_decisions\": unbiased_decisions,\n",
    "        \"decisions_count\": count_decisions(decisions),\n",
    "        \"position_swapped_decisions_count\": count_decisions(position_swapped_decisions),\n",
    "        \"name_swapped_decisions_count\": count_decisions(name_swapped_decisions),\n",
    "        \"unbiased_decisions_count\": count_decisions(unbiased_decisions),\n",
    "        \"decisions_win_rate\": get_win_rate(decisions),\n",
    "        \"position_swapped_decisions_win_rate\": get_win_rate(position_swapped_decisions),\n",
    "        \"name_swapped_decisions_win_rate\": get_win_rate(name_swapped_decisions),\n",
    "        \"unbiased_decisions_win_rate\": get_win_rate(unbiased_decisions),\n",
    "        \"position_comparison\": position_comparison,\n",
    "        \"name_comparison\": name_comparison,\n",
    "        \"position_bias\": position_bias,\n",
    "        \"name_bias\": name_bias,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    judge_model: str, input_1_jsonl: str, input_2_jsonl: str, question_template: str, n: int\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    judge_llm = LiteLLMModel(judge_model, max_tokens=4096, seed=1, temperature=0.6, top_p=0.95)\n",
    "\n",
    "    dataset_1 = load_dataset(\"json\", data_files=input_1_jsonl, split=\"train\", cache_dir=\".cache\")\n",
    "    dataset_2 = load_dataset(\"json\", data_files=input_2_jsonl, split=\"train\", cache_dir=\".cache\")\n",
    "    answers = convert_answers(dataset_1, dataset_2, question_template)\n",
    "\n",
    "    results = [run_experiment_once(judge_llm, answers) for _ in tqdm(range(n))]\n",
    "    return {key: pd.DataFrame([result[key] for result in results]) for key in results[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA_TEMPLATE = \"\"\"You are an expert in analyzing the text content and assigning finding the general type of persona that could be associated with such a way of expressing. Please use one or two sentences for the definition, but try to make it as fine-grained if input texts involve many detailed elements. The persona definition must go straight to the point, be assertive. The following are starts of persona definitions:\n",
    "A machine learning researcher...\n",
    "A pediatric nurse whose...\n",
    "An urban planner focused on...\n",
    "\n",
    "What is the likely profession, interest, or role of the person who would write or be interested in this text?\n",
    "\n",
    "## Text\n",
    "{text}\n",
    "\n",
    "Note:\n",
    "1. Your response should always start with \"ペルソナ:\".\n",
    "2. Your response should be one sentence. You should not include any notes or translations.\n",
    "3. 簡潔に日本語で回答してください。\n",
    "\"\"\"\n",
    "\n",
    "PERSONA_TEMPLATE_EN = \"\"\"You are an expert in analyzing the text content and assigning finding the general type of persona that could be associated with such a way of expressing. Please use one or two sentences for the definition, but try to make it as fine-grained if input texts involve many detailed elements. The persona definition must go straight to the point, be assertive. The following are starts of persona definitions:\n",
    "A machine learning researcher...\n",
    "A pediatric nurse whose...\n",
    "An urban planner focused on...\n",
    "\n",
    "What is the likely profession, interest, or role of the person who would write or be interested in this text?\n",
    "\n",
    "## Text\n",
    "{text}\n",
    "\n",
    "Note:\n",
    "1. Your response should always start with \"ペルソナ:\".\n",
    "2. Your response should be one sentence. You should not include any notes or translations.\n",
    "3. Please respond in English.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_answers(dataset_a: Dataset, dataset_b: Dataset, question_template: str) -> list[dict[str, str]]:\n",
    "    return [\n",
    "        {\n",
    "            \"question\": question_template.format(text=a[\"text\"]),\n",
    "            \"answer_a\": a[\"persona_answer\"],\n",
    "            \"answer_b\": b[\"persona_answer\"],\n",
    "        }\n",
    "        for a, b in zip(dataset_a, dataset_b)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:02:44 - DEBUG - __main__ - model: deepseek/deepseek-reasoner, max_tokens: 4096, seed: 1, temperature: 0.6, top_p: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:02:45 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-13 01:04:25 - DEBUG - __main__ - judging baseline...done: 100.47s\n",
      "2025-03-13 01:04:25 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-13 01:06:33 - DEBUG - __main__ - judging position swapped...done: 127.94s\n",
      "2025-03-13 01:06:33 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-13 01:07:51 - DEBUG - __main__ - judging name swapped...done: 77.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [05:06<35:43, 306.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:07:51 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-13 01:10:03 - DEBUG - __main__ - judging baseline...done: 131.89s\n",
      "2025-03-13 01:10:03 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-13 01:12:29 - DEBUG - __main__ - judging position swapped...done: 145.86s\n",
      "2025-03-13 01:12:29 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-13 01:14:27 - DEBUG - __main__ - judging name swapped...done: 118.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [11:42<35:54, 359.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:14:27 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-13 01:16:46 - DEBUG - __main__ - judging baseline...done: 138.53s\n",
      "2025-03-13 01:16:46 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-13 01:18:29 - DEBUG - __main__ - judging position swapped...done: 103.84s\n",
      "2025-03-13 01:18:29 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-13 01:19:45 - DEBUG - __main__ - judging name swapped...done: 75.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [16:59<28:20, 340.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:19:45 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-13 01:21:37 - DEBUG - __main__ - judging baseline...done: 112.40s\n",
      "2025-03-13 01:21:37 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-13 01:23:31 - DEBUG - __main__ - judging position swapped...done: 113.65s\n",
      "2025-03-13 01:23:31 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-13 01:24:49 - DEBUG - __main__ - judging name swapped...done: 78.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [22:04<21:44, 326.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:24:49 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-13 01:26:34 - DEBUG - __main__ - judging baseline...done: 104.82s\n",
      "2025-03-13 01:26:34 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-13 01:28:15 - DEBUG - __main__ - judging position swapped...done: 101.43s\n",
      "2025-03-13 01:28:15 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-13 01:29:34 - DEBUG - __main__ - judging name swapped...done: 78.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [26:49<15:33, 311.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:29:34 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-13 01:31:22 - DEBUG - __main__ - judging baseline...done: 108.26s\n",
      "2025-03-13 01:31:22 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-13 01:32:54 - DEBUG - __main__ - judging position swapped...done: 91.44s\n",
      "2025-03-13 01:32:54 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-13 01:34:24 - DEBUG - __main__ - judging name swapped...done: 90.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [31:39<10:08, 304.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:34:24 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-13 01:36:03 - DEBUG - __main__ - judging baseline...done: 99.09s\n",
      "2025-03-13 01:36:03 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-13 01:37:46 - DEBUG - __main__ - judging position swapped...done: 102.70s\n",
      "2025-03-13 01:37:46 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-13 01:38:57 - DEBUG - __main__ - judging name swapped...done: 71.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [36:12<04:53, 293.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:38:57 - DEBUG - __main__ - judging baseline...\n",
      "2025-03-13 01:40:26 - DEBUG - __main__ - judging baseline...done: 88.40s\n",
      "2025-03-13 01:40:26 - DEBUG - __main__ - judging position swapped...\n",
      "2025-03-13 01:42:17 - DEBUG - __main__ - judging position swapped...done: 110.91s\n",
      "2025-03-13 01:42:17 - DEBUG - __main__ - judging name swapped...\n",
      "2025-03-13 01:43:41 - DEBUG - __main__ - judging name swapped...done: 84.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [40:55<00:00, 306.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decisions':   0  1  2  3  4  5  6  7  8  9   ... 90 91 92 93 94 95 96 97 98 99\n",
       " 0  A  A  A  A  A  B  B  A  A  A  ...  A  A  B  A  A  B  A  B  A  A\n",
       " 1  A  A  A  A  A  B  B  A  A  A  ...  A  A  A  B  A  A  A  B  A  A\n",
       " 2  A  A  A  A  B  A  C  B  A  A  ...  A  A  A  B  A  B  A  B  B  A\n",
       " 3  A  A  A  A  A  A  B  B  A  A  ...  A  A  A  B  A  A  A  B  A  A\n",
       " 4  A  A  A  A  A  A  B  A  A  A  ...  A  A  B  A  A  A  A  B  B  A\n",
       " 5  A  A  A  A  A  A  B  B  A  A  ...  A  A  A  B  A  A  A  B  A  A\n",
       " 6  A  A  A  A  A  A  A  A  A  A  ...  A  A  A  B  A  B  A  B  A  A\n",
       " 7  A  A  A  A  A  A  B  B  A  A  ...  A  A  A  B  A  B  A  B  A  A\n",
       " \n",
       " [8 rows x 100 columns],\n",
       " 'position_swapped_decisions':   0  1  2  3  4  5  6  7  8  9   ... 90 91 92 93 94 95 96 97 98 99\n",
       " 0  A  A  A  A  A  A  B  B  A  A  ...  A  B  A  B  A  B  A  B  A  A\n",
       " 1  A  A  A  A  A  A  B  A  A  A  ...  A  A  A  B  A  B  A  B  A  A\n",
       " 2  A  A  A  A  A  A  B  B  A  A  ...  B  A  A  B  A  B  A  B  A  A\n",
       " 3  A  A  A  A  A  A  B  B  A  A  ...  A  B  A  B  A  A  A  B  B  A\n",
       " 4  A  A  A  A  A  A  A  B  A  A  ...  A  A  C  B  A  B  A  B  B  A\n",
       " 5  A  A  A  A  B  A  B  B  A  A  ...  B  B  A  B  A  B  A  B  A  A\n",
       " 6  A  A  A  A  A  A  A  A  A  A  ...  A  A  B  B  A  B  A  B  A  A\n",
       " 7  A  A  A  A  B  A  A  B  A  A  ...  A  B  A  B  A  A  A  B  B  A\n",
       " \n",
       " [8 rows x 100 columns],\n",
       " 'name_swapped_decisions':   0  1  2  3  4  5  6  7  8  9   ... 90 91 92 93 94 95 96 97 98 99\n",
       " 0  A  A  A  A  A  A  B  A  A  A  ...  A  A  A  B  A  B  A  B  A  A\n",
       " 1  A  A  A  A  A  A  B  A  A  A  ...  A  A  A  B  A  A  A  B  A  A\n",
       " 2  A  A  A  A  A  A  B  A  A  A  ...  A  A  A  A  A  B  A  B  A  A\n",
       " 3  A  A  A  A  A  A  A  A  A  A  ...  A  A  A  B  A  A  A  B  A  A\n",
       " 4  A  A  A  A  A  A  A  A  A  A  ...  A  A  A  A  A  A  A  B  A  A\n",
       " 5  A  A  A  A  A  A  B  A  A  A  ...  A  A  A  A  A  A  A  B  A  A\n",
       " 6  A  A  A  A  A  A  B  A  A  A  ...  A  A  A  B  A  A  A  B  A  A\n",
       " 7  A  A  A  A  A  A  A  A  A  A  ...  A  A  A  A  A  A  A  B  A  A\n",
       " \n",
       " [8 rows x 100 columns],\n",
       " 'unbiased_decisions':   0  1  2  3  4  5  6  7  8  9   ... 90 91 92 93 94 95 96 97 98 99\n",
       " 0  A  A  A  A  A  C  B  C  A  A  ...  A  C  C  C  A  B  A  B  A  A\n",
       " 1  A  A  A  A  A  C  B  A  A  A  ...  A  A  A  B  A  C  A  B  A  A\n",
       " 2  A  A  A  A  C  A  C  C  A  A  ...  C  A  A  C  A  B  A  B  C  A\n",
       " 3  A  A  A  A  A  A  C  C  A  A  ...  A  C  A  B  A  A  A  B  C  A\n",
       " 4  A  A  A  A  A  A  C  C  A  A  ...  A  A  C  C  A  C  A  B  C  A\n",
       " 5  A  A  A  A  C  A  B  C  A  A  ...  C  C  A  C  A  C  A  B  A  A\n",
       " 6  A  A  A  A  A  A  C  A  A  A  ...  A  A  C  B  A  C  A  B  A  A\n",
       " 7  A  A  A  A  C  A  C  C  A  A  ...  A  C  A  C  A  C  A  B  C  A\n",
       " \n",
       " [8 rows x 100 columns],\n",
       " 'decisions_count':     A   B  C\n",
       " 0  77  21  2\n",
       " 1  79  21  0\n",
       " 2  74  25  1\n",
       " 3  76  23  1\n",
       " 4  79  20  1\n",
       " 5  81  18  1\n",
       " 6  76  22  2\n",
       " 7  78  21  1,\n",
       " 'position_swapped_decisions_count':     A   B  C\n",
       " 0  67  31  2\n",
       " 1  65  33  2\n",
       " 2  71  27  2\n",
       " 3  68  31  1\n",
       " 4  65  32  3\n",
       " 5  69  31  0\n",
       " 6  74  25  1\n",
       " 7  66  33  1,\n",
       " 'name_swapped_decisions_count':     A   B  C\n",
       " 0  87  13  0\n",
       " 1  90  10  0\n",
       " 2  88  12  0\n",
       " 3  90  10  0\n",
       " 4  85  14  1\n",
       " 5  89  10  1\n",
       " 6  90  10  0\n",
       " 7  87  13  0,\n",
       " 'unbiased_decisions_count':     A   B   C\n",
       " 0  64  10  26\n",
       " 1  58   8  34\n",
       " 2  62  10  28\n",
       " 3  62   9  29\n",
       " 4  61  11  28\n",
       " 5  64   8  28\n",
       " 6  65   8  27\n",
       " 7  58   8  34,\n",
       " 'decisions_win_rate':           A         B\n",
       " 0  0.785714  0.214286\n",
       " 1  0.790000  0.210000\n",
       " 2  0.747475  0.252525\n",
       " 3  0.767677  0.232323\n",
       " 4  0.797980  0.202020\n",
       " 5  0.818182  0.181818\n",
       " 6  0.775510  0.224490\n",
       " 7  0.787879  0.212121,\n",
       " 'position_swapped_decisions_win_rate':           A         B\n",
       " 0  0.683673  0.316327\n",
       " 1  0.663265  0.336735\n",
       " 2  0.724490  0.275510\n",
       " 3  0.686869  0.313131\n",
       " 4  0.670103  0.329897\n",
       " 5  0.690000  0.310000\n",
       " 6  0.747475  0.252525\n",
       " 7  0.666667  0.333333,\n",
       " 'name_swapped_decisions_win_rate':           A         B\n",
       " 0  0.870000  0.130000\n",
       " 1  0.900000  0.100000\n",
       " 2  0.880000  0.120000\n",
       " 3  0.900000  0.100000\n",
       " 4  0.858586  0.141414\n",
       " 5  0.898990  0.101010\n",
       " 6  0.900000  0.100000\n",
       " 7  0.870000  0.130000,\n",
       " 'unbiased_decisions_win_rate':           A         B\n",
       " 0  0.864865  0.135135\n",
       " 1  0.878788  0.121212\n",
       " 2  0.861111  0.138889\n",
       " 3  0.873239  0.126761\n",
       " 4  0.847222  0.152778\n",
       " 5  0.888889  0.111111\n",
       " 6  0.890411  0.109589\n",
       " 7  0.878788  0.121212,\n",
       " 'position_comparison':                                     count                        bias\n",
       " 0  {'A': 64, 'B': 18, 'C': 0, 'bias': 18}  {'first': 15, 'second': 3}\n",
       " 1  {'A': 59, 'B': 15, 'C': 0, 'bias': 26}  {'first': 20, 'second': 6}\n",
       " 2  {'A': 63, 'B': 16, 'C': 0, 'bias': 21}  {'first': 12, 'second': 9}\n",
       " 3  {'A': 62, 'B': 18, 'C': 0, 'bias': 20}  {'first': 14, 'second': 6}\n",
       " 4  {'A': 62, 'B': 15, 'C': 0, 'bias': 23}  {'first': 18, 'second': 5}\n",
       " 5  {'A': 65, 'B': 15, 'C': 0, 'bias': 20}  {'first': 16, 'second': 4}\n",
       " 6  {'A': 66, 'B': 15, 'C': 1, 'bias': 18}  {'first': 10, 'second': 8}\n",
       " 7  {'A': 60, 'B': 15, 'C': 1, 'bias': 24}  {'first': 18, 'second': 6},\n",
       " 'name_comparison':                                     count                        bias\n",
       " 0  {'A': 75, 'B': 10, 'C': 0, 'bias': 15}  {'first': 12, 'second': 3}\n",
       " 1   {'A': 77, 'B': 8, 'C': 0, 'bias': 15}  {'first': 13, 'second': 2}\n",
       " 2  {'A': 73, 'B': 10, 'C': 0, 'bias': 17}  {'first': 15, 'second': 2}\n",
       " 3  {'A': 76, 'B': 10, 'C': 0, 'bias': 14}  {'first': 14, 'second': 0}\n",
       " 4  {'A': 77, 'B': 12, 'C': 0, 'bias': 11}   {'first': 9, 'second': 2}\n",
       " 5   {'A': 78, 'B': 8, 'C': 0, 'bias': 14}  {'first': 11, 'second': 3}\n",
       " 6   {'A': 74, 'B': 8, 'C': 0, 'bias': 18}  {'first': 16, 'second': 2}\n",
       " 7   {'A': 74, 'B': 9, 'C': 0, 'bias': 17}  {'first': 13, 'second': 4},\n",
       " 'position_bias':    consistent  first  second\n",
       " 0        0.82   0.15    0.03\n",
       " 1        0.74   0.20    0.06\n",
       " 2        0.79   0.12    0.09\n",
       " 3        0.80   0.14    0.06\n",
       " 4        0.77   0.18    0.05\n",
       " 5        0.80   0.16    0.04\n",
       " 6        0.82   0.10    0.08\n",
       " 7        0.76   0.18    0.06,\n",
       " 'name_bias':    consistent  first  second\n",
       " 0        0.85   0.12    0.03\n",
       " 1        0.85   0.13    0.02\n",
       " 2        0.83   0.15    0.02\n",
       " 3        0.86   0.14    0.00\n",
       " 4        0.89   0.09    0.02\n",
       " 5        0.86   0.11    0.03\n",
       " 6        0.82   0.16    0.02\n",
       " 7        0.83   0.13    0.04}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"DEEPSEEK_API_BASE\"] = \"http://192.168.11.43:39877/v1\"\n",
    "judge_model = \"deepseek/deepseek-reasoner\"\n",
    "# input_1_jsonl = \"eval/data/persona-rinna-r1-3407.jsonl\"\n",
    "# input_2_jsonl = \"eval/data/persona-gpt-4o-mini-3407.jsonl\"\n",
    "# question_template = PERSONA_TEMPLATE\n",
    "input_1_jsonl = \"eval/data/persona-rinna-r1-en-3407.jsonl\"\n",
    "input_2_jsonl = \"eval/data/persona-gpt-4o-mini-en-3407.jsonl\"\n",
    "question_template = PERSONA_TEMPLATE_EN\n",
    "n = 8\n",
    "\n",
    "dfs = run_experiment(judge_model, input_1_jsonl, input_2_jsonl, question_template, n)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"eval/data\"\n",
    "# prefix = \"results-persona-en-rinna-vs-gpt4-3407\"\n",
    "prefix = \"results-persona-rinna-vs-gpt4-3407\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for key, df in dfs.items():\n",
    "    df.to_csv(os.path.join(output_dir, f\"{prefix}-{key}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from the saved results\n",
    "dfs = {}\n",
    "for key in [\n",
    "    \"decisions\",\n",
    "    \"position_swapped_decisions\",\n",
    "    \"name_swapped_decisions\",\n",
    "    \"unbiased_decisions\",\n",
    "    \"decisions_count\",\n",
    "    \"position_swapped_decisions_count\",\n",
    "    \"name_swapped_decisions_count\",\n",
    "    \"unbiased_decisions_count\",\n",
    "    \"decisions_win_rate\",\n",
    "    \"position_swapped_decisions_win_rate\",\n",
    "    \"name_swapped_decisions_win_rate\",\n",
    "    \"unbiased_decisions_win_rate\",\n",
    "    \"position_comparison\",\n",
    "    \"name_comparison\",\n",
    "    \"position_bias\",\n",
    "    \"name_bias\",\n",
    "]:\n",
    "    dfs[key] = pd.read_csv(os.path.join(output_dir, f\"{prefix}-{key}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.116883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.085714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.094595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.102564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.093333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B\n",
       "0  0.883117  0.116883\n",
       "1  0.914286  0.085714\n",
       "2  0.888889  0.111111\n",
       "3  0.905405  0.094595\n",
       "4  0.909091  0.090909\n",
       "5  0.933333  0.066667\n",
       "6  0.897436  0.102564\n",
       "7  0.906667  0.093333"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"unbiased_decisions_win_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(A    0.904778\n",
       " B    0.095222\n",
       " dtype: float64,\n",
       " A    0.015602\n",
       " B    0.015602\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"unbiased_decisions_win_rate\"].mean(), dfs[\"unbiased_decisions_win_rate\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(consistent    0.78750\n",
       " first         0.15375\n",
       " second        0.05875\n",
       " dtype: float64,\n",
       " consistent    0.028661\n",
       " first         0.033354\n",
       " second        0.019594\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"position_bias\"].mean(), dfs[\"position_bias\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(consistent    0.84875\n",
       " first         0.12875\n",
       " second        0.02250\n",
       " dtype: float64,\n",
       " consistent    0.022321\n",
       " first         0.022321\n",
       " second        0.011650\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"name_bias\"].mean(), dfs[\"name_bias\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A   B   C\n",
       "0  64  10  26\n",
       "1  58   8  34\n",
       "2  62  10  28\n",
       "3  62   9  29\n",
       "4  61  11  28\n",
       "5  64   8  28\n",
       "6  65   8  27\n",
       "7  58   8  34"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"unbiased_decisions_count\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
